<div class="whitebox textbox col-12">
  <div class="text-center">
    <h4>Posts</h4>
  </div>
</div>
<div class='gap col-xs-12'></div>  
<div class="whitebox textbox col-12" style='word-wrap: break-word;'>
  <div class="text-center">
    <h4>Publications</h4>
  </div>
  <hr>
  <ol class="bibliography"><li>Automated System Health and Performance Benchmarking Platform: High Performance Computing Test Harness with Jenkins
  <br>

<!--
<span id="FLAT17">Voss, J., Garcia, J. A., Proctor, W. C., &amp; Evans, R. T. (2017). Automated System Health and Performance Benchmarking Platform: High Performance Computing Test Harness with Jenkins. In <i>Proceedings of the HPC Systems Professionals Workshop</i> (pp. 1:1–1:8). New York, NY, USA: ACM. https://doi.org/10.1145/3155105.3155106</span>

    <a href="http://dx.doi.org/10.1145/3155105.3155106">http://dx.doi.org/10.1145/3155105.3155106</a>


<br>

<p>Datacenters have a growing need to monitor and maintain
		complicated computing machines and verify their systems are functioning
		at a high level. In order to achieve this goal, it is critical that
		administrators are able to readily verify basic user operations and
		survey system performance, quickly discerning when a configuration is
		sub-optimal. We have created a system health and performance monitoring
		tool that enables tracking both health and historical performance. The
		tool presents this data visually and enables the identification of
		realized and potential problems intuitively. This tool leverages Slurm, a
		workload manager common in, yet critical to High Performance Computing
		workflows. We construct the tool around Jenkins, a popular and
		well-supported testing automation framework which has been used in recent
		system health and regression testing, as well as PyTest, an
		assertion-driven Python unit test framework, after evaluating several
		potential automation tools and testing frameworks. This project develops
		a test harness for the Texas Advanced Computing Center to run multiple
		extendable suites of benchmarking and system health applications
		demonstrated on the Stampede2 and Lonestar5 HPC systems. The applications
		chosen to run within the test harness include existing in-house
		benchmarks, such as the Performance Assessment Workbench, and community
		benchmarks, e.g. STREAM, in addition to newly created system health
		monitoring scripts.</p>


 <pre>@inproceedings{FLAT17,
  author = {Voss, Joseph and Garcia, Joe A. and Proctor, W. Cyrus and Evans, R. Todd},
  title = {Automated System Health and Performance Benchmarking Platform: High Performance Computing Test Harness with Jenkins},
  booktitle = {Proceedings of the HPC Systems Professionals Workshop},
  series = {HPCSYSPROS'17},
  year = {2017},
  isbn = {978-1-4503-5128-7},
  location = {Denver, CO, USA},
  pages = {1:1--1:8},
  articleno = {1},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/3155105.3155106},
  doi = {10.1145/3155105.3155106},
  acmid = {3155106},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {HPC, Performance Benchmarking, System Health, System Monitoring}
}
</pre>

    <script>
value.to_s.gsub(URI.regexp(['http','https','ftp'])) { |c| "<a
    href=\"#{$&}\">#{$&}</a>" }
    </script> -->
<a class="details" href="/bibliography/FLAT17.html">Details</a></li>
<li>Student Cluster Competition 2016 reproducibility challenge: Genomic partitioning with ParConnect

<!--
<span id="ABABAO2017">Ababao, R., Garcia, J. A., Voss, J., Proctor, W. C., &amp; Evans, R. T. (2017). Student Cluster Competition 2016 reproducibility challenge: Genomic partitioning with ParConnect. <i>Parallel Computing</i>. https://doi.org/10.1016/j.parco.2017.07.002</span>

    <a href="http://dx.doi.org/10.1016/j.parco.2017.07.002">http://dx.doi.org/10.1016/j.parco.2017.07.002</a>


<br>

<p>As part of a reproducibility initiative from the Student Cluster
    Competition 2016, specific results and trends presented in “A parallel
    connectivity algorithm for de Bruijn graphs in metagenomic applications” were similarly produced and verified. The general lack of reproducibility within the scientific community is a known issue, but few have the time, resources, or incentives to fully address it. Motivation for reproducibility resides in the need to independently validate previous research claims and test the difficulty or ease with which these claims may be reasserted. This fundamental tenant becomes ever more important, particularly due the prohibitive simulation cost and data complexity currently associated with metagenomics. The algorithms in the aforementioned article provide a scalable, distributed memory solution to the problem of assembling and labeling connected components in graphs associated with metagenomic samples. We aim to verify four of the components demonstrated by this article; namely, the deterministic countability of connected components in the data sets used, the computation to communication ratio of different work-balanced parallel algorithm implementations, the results obtained from said algorithm implementations, and the scaling behavior of the algorithms as the number of MPI processes are increased.</p>


 <pre>@article{ABABAO2017,
  title = {Student Cluster Competition 2016 reproducibility challenge: Genomic partitioning with ParConnect},
  journal = {Parallel Computing},
  volume = {},
  number = {},
  pages = {},
  year = {2017},
  note = {},
  issn = {0167-8191},
  doi = {10.1016/j.parco.2017.07.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0167819117300947},
  author = {Ababao, Rainier and Garcia, Joe A. and Voss, Joseph and Proctor, W. Cyrus and Evans, R. Todd},
  keywords = {Student cluster competition}
}
</pre>

    <script>
value.to_s.gsub(URI.regexp(['http','https','ftp'])) { |c| "<a
    href=\"#{$&}\">#{$&}</a>" }
    </script> -->
<a class="details" href="/bibliography/ABABAO2017.html">Details</a></li></ol>
</div>
